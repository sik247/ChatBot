{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Practice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Before running the application, you need to set up your OpenAI API key. This key is used to authenticate with the OpenAI API and use their language models.\n",
    "\n",
    "Follow these steps to set up your API key:\n",
    "\n",
    "1. If you haven't already, sign up for an account on the [OpenAI website](https://www.openai.com/).\n",
    "2. Once you have an account, you can find your API key in your account settings.\n",
    "3. Copy your API key.\n",
    "4. In your local environment where you're running this application, set an environment variable named `OPENAI_API_KEY` with the value of your API key. \n",
    "\n",
    "Here's how you can do this:\n",
    "\n",
    "- On Unix-based systems (like Linux or MacOS), you can use the `export` command in your terminal:\n",
    "\n",
    "    ```bash\n",
    "    export OPENAI_API_KEY='your-api-key'\n",
    "    ```\n",
    "\n",
    "- On Windows, you can use the `setx` command in the Command Prompt:\n",
    "\n",
    "    ```cmd\n",
    "    setx OPENAI_API_KEY \"your-api-key\"\n",
    "    ```\n",
    "\n",
    "Replace `'your-api-key'` or `\"your-api-key\"` with the actual API key that you copied from the OpenAI website.\n",
    "\n",
    "Now, when you run the application, it will use your API key to authenticate with the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # load environment variables from .env file\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")  # get API key from environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature value dictates how creative the model is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrykang/Desktop/LLM/Project1/venv/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(openai_api_key = os.environ[\"OPENAI_API_KEY\"], temperature = 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The capital of South Korea is Seoul.\n"
     ]
    }
   ],
   "source": [
    "text  = \"What is the capital of South Korea\"\n",
    "print(llm.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # load environment variables from .env file\n",
    "\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")  # get API token from environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrykang/Desktop/LLM/Project1/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "llm_huggingface= HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\": 0.6, \"max_length\": 64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrykang/Desktop/LLM/Project1/venv/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seoul\n"
     ]
    }
   ],
   "source": [
    "output = llm_huggingface.predict(\"Can you tell me the capital of South Korea?\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love you i love\n"
     ]
    }
   ],
   "source": [
    "output1 = llm_huggingface.predict(\"Can you write me a poem?\")\n",
    "print(output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates \n",
    "\n",
    "Prompt templates are used to structure the input to a language model in a consistent and controlled manner. They are particularly useful when working with AI models like GPT-3 or GPT-4, which generate text based on a given prompt.\n",
    "\n",
    "In the code you provided, a `PromptTemplate` object is being created with an input variable `country` and a template string \"What is the capital of {country}?\". This template can then be formatted with different values for `country` to generate different prompts.\n",
    "\n",
    "For example, `prompt_template.format(country = \"South Korea\")` would generate the prompt \"What is the capital of South Korea?\".\n",
    "\n",
    "By using a prompt template, you can easily generate a wide variety of prompts without having to manually construct each one. This can make your code cleaner and more efficient, especially when dealing with a large number of prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate \n",
    "#choose how are input and how are outputs are made "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the capital of South Korea?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(input_variables =['country'],\n",
    "                                 template = \"What is the capital of {country}?\",)\n",
    "prompt_template.format(country = \"South Korea\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains - Combine multiple things then Execute \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auckland\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm = llm_huggingface, prompt= prompt_template)\n",
    "print(chain.run(\"New Zealand\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_community.llms.openai.OpenAI'>\n"
     ]
    }
   ],
   "source": [
    "print(type(llm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Multiple Chains Using Simple Sequential Chains \n",
    "\n",
    "In the process of generating responses or carrying out tasks, we often need to perform a series of operations in a specific order. This is where Sequential Chains come into play. Sequential Chains allow us to combine multiple operations or 'chains' in a sequence, where the output of one chain serves as the input to the next.\n",
    "\n",
    "For instance, in the context of a language model, we might have a chain that generates a response based on a specific prompt, and another chain that takes this response and further refines or processes it. By combining these chains in a sequence, we can automate the process of generating complex responses.\n",
    "\n",
    "In our code, we use the `LLMChain` class to create individual chains with specific prompts. These chains are then combined using the `SimpleSequentialChain` class to create a sequence of operations that can be run with a single command. This allows us to generate multi-step conversations or carry out complex tasks with ease.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_prompt = PromptTemplate(input_variables = ['country'],\n",
    "template = \"Please tell me the capital of {country}\")\n",
    "\n",
    "capital_chain = LLMChain(llm = llm_huggingface, prompt = capital_prompt)\n",
    "\n",
    "famous_template = PromptTemplate(input_variables = ['capital'],\n",
    "template = \"Can you give me 10 unique facts about {capital}\")\n",
    "\n",
    "famous_chain = LLMChain(llm = llm_huggingface, prompt = famous_template)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Auckland is the capital of New Zealand'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We need import because we are chaining output of one chain as input of another \n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "chain = SimpleSequentialChain(chains=[capital_chain, famous_chain])\n",
    "chain.run(\"New Zealand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_prompt = PromptTemplate(input_variables = ['country'],\n",
    "template = \"Please tell me the capital of {country}\")\n",
    "\n",
    "capital_chain = LLMChain(llm = llm_huggingface, prompt = capital_prompt,output_key = \"capital\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "famous_template = PromptTemplate(input_variables = ['capital'],\n",
    "template = \"What are some great places to visit {capital}\")\n",
    "\n",
    "famous_chain = LLMChain(llm = llm_huggingface, prompt = famous_template, output_key=\"facts\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Chain\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_prompt = PromptTemplate(input_variables = ['country'],\n",
    "template = \"Please tell me the capital of {country}\")\n",
    "\n",
    "capital_chain = LLMChain(llm = llm_huggingface, prompt = capital_prompt,output_key = \"capital\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "famous_template = PromptTemplate(input_variables = ['capital'],\n",
    "template = \"What are some great places to visit {capital}\")\n",
    "\n",
    "famous_chain = LLMChain(llm = llm_huggingface, prompt = famous_template, output_key=\"facts\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "chain = SequentialChain(chains=[capital_chain, famous_chain],\n",
    "                        input_variables = ['country'],\n",
    "                        output_variables = ['capital', 'facts'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': 'South Korea',\n",
       " 'capital': 'seoul',\n",
       " 'facts': 'Seoul National Museum'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain({\"country\": \"South Korea\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message Handling in Langchain\n",
    "\n",
    "In our system, we handle three types of messages: human-generated messages, system messages, and AI-generated messages. These are represented by the `HumanMessage`, `SystemMessage`, and `AIMessage` classes respectively, all of which are imported from the `langchain.schema` module.\n",
    "\n",
    "- `HumanMessage`: Represents messages that are input by a human user. These could be commands, queries, or any other form of input that the system needs to respond to.\n",
    "\n",
    "- `SystemMessage`: Represents system-level messages. These could be error messages, status updates, or any other kind of message that the system generates to communicate its state or any issues it encounters.\n",
    "\n",
    "- `AIMessage`: Represents messages that are generated by an AI model. These are typically responses to human messages, generated based on a specific prompt and the internal logic of the AI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrykang/Desktop/LLM/Project1/venv/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "chatllm = ChatOpenAI(openai_api_key = os.environ[\"OPENAI_API_KEY\"], temperature = 0.6, model = 'gpt-3.5-turbo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure, here are some cat jokes for you:\\n\\n1. Why was the cat sitting on the computer?\\n   Because it wanted to keep an eye on the mouse!\\n\\n2. What do you call a pile of cats?\\n   A meowtain!\\n\\n3. How does a cat end a fight?\\n   They hiss and make up!\\n\\n4. What do you call a cat that likes to swim?\\n   A catfish!\\n\\n5. Why did the cat join the Red Cross?\\n   Because it wanted to be a first-aid kit!\\n\\nI hope these cat jokes made you smile!', response_metadata={'token_usage': {'completion_tokens': 118, 'prompt_tokens': 22, 'total_tokens': 140}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-97243f68-d5ef-44ea-88c1-aa28eed38638-0')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatllm([ \n",
    "SystemMessage(content = \"You are a comedian AI Assistant\"),\n",
    "HumanMessage(content = \"Please provide jokes about cats\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template + LLM + Output Parsers\n",
    "\n",
    "In the first line of the `langchain.ipynb` file, we are importing the `langchain` module. This module likely contains the main functionality for the language chain system, including classes and functions for creating and managing chains of language model operations.\n",
    "\n",
    "However, the import statement is incomplete. It should specify what exactly is being imported from the `langchain` module. For example, it might look something like this:\n",
    "\n",
    "```python\n",
    "from langchain import LLMChain, PromptTemplate, SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Necessary Modules for Chat-based Language Model Interaction\n",
    "\n",
    "In our code, we import several modules to handle different aspects of interacting with a chat-based language model:\n",
    "\n",
    "- `from langchain.chat_models import ChatOpenAI`: Imports the `ChatOpenAI` class, which is likely a utility class for interacting with the OpenAI API in a chat-based context.\n",
    "\n",
    "- `from langchain.prompts.chat import ChatPromptTemplate`: Imports the `ChatPromptTemplate` class, which is used to structure and manage the prompts that are sent to the chat model.\n",
    "\n",
    "- `from langchain.schema import BaseOutputParser`: Imports the `BaseOutputParser` class, which is likely a base class for output parsers. These parsers are used to process and interpret the output of the chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "from langchain.schema import BaseOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Commaseperatedoutput(BaseOutputParser):\n",
    "    def parse(self, text:str):\n",
    "        return text.strip().split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a helpful assistant. When the use given any input, you should provide a 5 synonyms of the input.\"\n",
    "human_template = \"{text}\"\n",
    "chatprompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chatprompt | chatllm | Commaseperatedoutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. insolent\\n2. cheeky\\n3. brazen\\n4. audacious\\n5. disrespectful']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"text\": \"impudent\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
